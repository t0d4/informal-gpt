{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental implementation of Informal-GPT\n",
    "\n",
    "This notebook will finetune GPT-2 medium model to talk in informal way.\n",
    "\n",
    "Will use\n",
    "\n",
    "- [s-nlp/roberta-base-formality-ranker](https://huggingface.co/s-nlp/roberta-base-formality-ranker) as a reward model\n",
    "- [gpt2-medium](https://huggingface.co/gpt2-medium) as a autoregressive text generator\n",
    "- 60% [binhgiangnguyendanh/reddit_casual_conversation_for_alpaca_lora](https://huggingface.co/datasets/binhgiangnguyendanh/reddit_casual_conversation_for_alpaca_lora) and 40% [wikitext](https://huggingface.co/datasets/wikitext) as a text source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import re\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# tqdm.pandas()\n",
    "\n",
    "# Hugging Face transformers\n",
    "from transformers import pipeline, AutoTokenizer, PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "\n",
    "# TRL library\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "# IDs of models to use\n",
    "REWARD_MODEL_ID = \"s-nlp/roberta-base-formality-ranker\"\n",
    "TEXT_GENERATION_MODEL_ID = \"gpt2-medium\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: No names found, cannot describe anything.\n"
     ]
    }
   ],
   "source": [
    "GPT_BATCH_SIZE = 330\n",
    "BERT_BATCH_SIZE = 16\n",
    "\n",
    "# os.environ[\"WANDB_PROJECT\"] = \"informal-gpt\"\n",
    "ppo_config = PPOConfig(\n",
    "    batch_size=GPT_BATCH_SIZE,\n",
    "    model_name=TEXT_GENERATION_MODEL_ID,\n",
    "    learning_rate=1.41e-5,  # NOTE: This parameter is taken from OpenAI's paper: \"Fine-Tuning Language Models from Human Preferences\"\n",
    "    log_with=\"wandb\",\n",
    ")\n",
    "datetime_str = str(datetime.datetime.now()).replace(\" \", \"_\").replace(\":\", \"-\")\n",
    "checkpoint_dir_name = f\"gpt2-medium-and-bert-based-formality-ranker-{datetime_str}\"\n",
    "\n",
    "bert_kwargs = {\n",
    "    \"return_all_scores\": True,  # make BERT return scores for all classes, not only the most likely one\n",
    "    \"function_to_apply\": \"none\",\n",
    "    \"batch_size\": BERT_BATCH_SIZE,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "connect to wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mt0d4\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/toda/informal-gpt/experiments/wandb/run-20230720_012509-bh0vu8vd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/t0d4/informal-gpt/runs/bh0vu8vd' target=\"_blank\">gpt2-medium-and-bert-based-formality-ranker-2023-07-20_01-25-07.925859</a></strong> to <a href='https://wandb.ai/t0d4/informal-gpt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/t0d4/informal-gpt' target=\"_blank\">https://wandb.ai/t0d4/informal-gpt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/t0d4/informal-gpt/runs/bh0vu8vd' target=\"_blank\">https://wandb.ai/t0d4/informal-gpt/runs/bh0vu8vd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/t0d4/informal-gpt/runs/bh0vu8vd?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f613e07c460>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(\n",
    "    project=\"informal-gpt\",\n",
    "    name=checkpoint_dir_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare **wikitext** dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define function for dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(\n",
    "    dataset_size: int,\n",
    "    tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],\n",
    "    input_min_text_length: int = 6,\n",
    "    input_max_text_length: int = 10,\n",
    ") -> DataLoader:\n",
    "    \n",
    "    ds_reddit = load_dataset(\"binhgiangnguyendanh/reddit_casual_conversation_for_alpaca_lora\", split=\"train\")\n",
    "    collected_texts = []\n",
    "    for text in ds_reddit[\"input\"] + ds_reddit[\"output\"]:\n",
    "        if not collected_texts:\n",
    "            collected_texts.append(text)\n",
    "        else:\n",
    "            previous_text = collected_texts[-1]\n",
    "            if text[:20] != previous_text[:20]:\n",
    "                collected_texts.append(text)\n",
    "    \n",
    "    # delete mentions like \"@user\" and resulting empty strings\n",
    "    split_pattern = re.compile(r'@[^\\s]+')\n",
    "    sanitized_userwise_texts = []\n",
    "    for whole_text in tqdm(collected_texts):\n",
    "        userwise_texts = re.split(split_pattern, whole_text)\n",
    "        userwise_texts = [re.sub(split_pattern, \"\", t) for t in userwise_texts]\n",
    "        userwise_texts = [t for t in userwise_texts if not (t.isspace() or t == \"\")]\n",
    "        userwise_texts = [t for t in userwise_texts if len(t) > 60]\n",
    "        sanitized_userwise_texts += userwise_texts\n",
    "    ds_reddit = Dataset.from_dict({\"text\": sanitized_userwise_texts})\n",
    "    ds_reddit = ds_reddit.shuffle()\n",
    "\n",
    "    ds_wiki = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"train\")\n",
    "    ds_wiki = ds_wiki.filter(\n",
    "        function=lambda item: all([ch not in item[\"text\"] for ch in [\"@\", \"(\", \")\", \"<\", \">\", \"/\", \";\", \":\", \"-\", \"=\"]]),  # only use sentences whose length is more than 150 characters\n",
    "        batched=False,\n",
    "        num_proc=8,\n",
    "    )\n",
    "    ds_wiki = ds_wiki.shuffle()\n",
    "    sanitized_wiki_texts = []\n",
    "    for text in ds_wiki[\"text\"]:\n",
    "        if not (text.isspace() or text == \"\"):\n",
    "            sanitized_wiki_texts.append(text)\n",
    "    ds_wiki = Dataset.from_dict({\"text\": sanitized_wiki_texts[:int(len(sanitized_userwise_texts)*0.66)]})\n",
    "\n",
    "    def tokenize(item):\n",
    "        \"\"\"\n",
    "            item[\"input_ids\"] will be a sequence of first `get_input_size()` tokens converted from the input text.\n",
    "            item[\"query\"] will be a human-readable raw text decoded from item[\"input_ids\"]\n",
    "        \"\"\"\n",
    "        get_input_size = LengthSampler(\n",
    "            min_value=input_min_text_length,\n",
    "            max_value=input_max_text_length,\n",
    "        )\n",
    "        item[\"input_ids\"] = tokenizer.encode(text=item[\"text\"], truncation=True, max_length=1024)[:get_input_size()]\n",
    "        item[\"query\"] = tokenizer.decode(token_ids=item[\"input_ids\"])\n",
    "        return item\n",
    "    \n",
    "    ds_reddit = ds_reddit.map(\n",
    "        function=tokenize,\n",
    "        batched=False,\n",
    "        num_proc=8,\n",
    "    )\n",
    "    ds_wiki = ds_wiki.map(\n",
    "        function=tokenize,\n",
    "        batched=False,\n",
    "        num_proc=8,\n",
    "    )\n",
    "    \n",
    "    ds = concatenate_datasets([ds_reddit, ds_wiki])\n",
    "    ds = ds.shuffle()\n",
    "    \n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instantiate tokenizer and do preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset parquet (/home/toda/.cache/huggingface/datasets/binhgiangnguyendanh___parquet/binhgiangnguyendanh--reddit_casual_conversation_for_alpaca_lora-5bac28dd8ea69bc6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|██████████| 11091/11091 [00:00<00:00, 144726.11it/s]\n",
      "WARNING:datasets.builder:Found cached dataset wikitext (/home/toda/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/toda/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-7709eae69915190b_*_of_00008.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ead02878b9e44699da624385cc4d8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/9883 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92baf2b70c44b9086a1cbb609b0183b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reference: imdb dataset's train split contains 24895 rows\n",
    "DATASET_SIZE = 30000\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=ppo_config.model_name,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # use <EOS> token for padding\n",
    "\n",
    "dataset = build_dataset(\n",
    "    dataset_size=DATASET_SIZE,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "def dataset_collator(data: List[Dict[str, Any]]) -> Dict[str, List[Any]]:\n",
    "    # Data is like list of dictionaries.\n",
    "    # We format them as a batch\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'input_ids', 'query'],\n",
      "    num_rows: 21883\n",
      "})\n",
      "{'input_ids': [tensor([30655,  2142, 26100,  7907, 25370,   254]),\n",
      "               tensor([  383, 48665,   282, 32536,   286,   262,  7439, 41416]),\n",
      "               tensor([ 9870,   502,    11, 38074, 15378,    11]),\n",
      "               tensor([ 818, 1029, 1524,  314, 3214,  287,  351]),\n",
      "               tensor([  383, 19955, 13399,  1061,   360,  3565,   705,  2878,   870])],\n",
      " 'query': [' Yugoslav Partisans captured Š',\n",
      "           ' The Patriarchal Cathedral of the Holy Ascension',\n",
      "           ' Trust me, Aunt Lisa,',\n",
      "           'In high school I fell in with',\n",
      "           \" The medieval texts follow Dares'structuring\"],\n",
      " 'text': [' Yugoslav Partisans captured Šibenik and Zadar by 3 November 1944 , '\n",
      "          'but the war in the Adriatic continued until April 1945 . Allied '\n",
      "          'destroyers never engaged large Kriegsmarine vessels in the Adriatic '\n",
      "          'after November 1944 . Dwindling German naval assets in the area '\n",
      "          'resulted in limited action , while the last recorded loss was TA 45 '\n",
      "          'torpedoed by Royal Navy MTBs in April . Only four Kriegsmarine '\n",
      "          'ships survived to be captured or scuttled when German forces in '\n",
      "          'Italy surrendered at the end of April to advancing forces of the '\n",
      "          'British 8th Army . \\n',\n",
      "          ' The Patriarchal Cathedral of the Holy Ascension of God is located '\n",
      "          'on top of the Tsarevets hill , overlooking the modern city of '\n",
      "          'Veliko Tarnovo . The church was part of a group of buildings which '\n",
      "          'constituted the seat of the Bulgarian Patriarchate and acted as the '\n",
      "          \"city and the country 's main cathedral . The patriarchate on \"\n",
      "          'Tsarevets was a fortress of its own , with two defensive towers and '\n",
      "          'an entrance on its west wall . The Patriarchal Cathedral stood in '\n",
      "          'the middle of its courtyard . \\n',\n",
      "          ' Trust me, Aunt Lisa, you are cool as hell to those kids. I had '\n",
      "          'some family friends who were unmarried and without children, and '\n",
      "          'they taught me a lot of things my parents were too distanced from '\n",
      "          'to teach me. As another commenter said, we all have our roles to '\n",
      "          'fulfill in life, and these aren’t always related to children at '\n",
      "          'all. It’s nice that your role still means you get to be a part of '\n",
      "          'their life, while also focusing on doing what’s best for you! It '\n",
      "          'sounds like you are super important to your family, so I’m glad you '\n",
      "          'got the affirmation you needed and deserve. \\n',\n",
      "          'In high school I fell in with the metal heads. They were the nicest '\n",
      "          'kids and a lot more feeling and thinking than say the basic jocks '\n",
      "          'or what have you. As a girl I was teased for moving “to the dark '\n",
      "          'side” in 10th grade but thems were my people. God, school was so '\n",
      "          'damn cliquey...',\n",
      "          \" The medieval texts follow Dares ' structuring of the narrative in \"\n",
      "          'describing Troilus after his parents and four royal brothers Hector '\n",
      "          ', Paris , Deiphobus and Helenus . \\n']}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "print(dataset)\n",
    "pprint(dataset[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained text generation models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    pretrained_model_name_or_path=ppo_config.model_name,\n",
    ")\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    pretrained_model_name_or_path=ppo_config.model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:gsgvp7ni) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">megatron-gpt2-medium-and-bert-based-formality-ranker</strong> at: <a href='https://wandb.ai/t0d4/informal-gpt/runs/gsgvp7ni' target=\"_blank\">https://wandb.ai/t0d4/informal-gpt/runs/gsgvp7ni</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230719_020146-gsgvp7ni/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:gsgvp7ni). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb65d0dafe954fed87dfa112869bb2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01667026193366231, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/toda/informal-gpt/wandb/run-20230719_020231-ox49wvqk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/t0d4/trl/runs/ox49wvqk' target=\"_blank\">proud-planet-3</a></strong> to <a href='https://wandb.ai/t0d4/trl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/t0d4/trl' target=\"_blank\">https://wandb.ai/t0d4/trl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/t0d4/trl/runs/ox49wvqk' target=\"_blank\">https://wandb.ai/t0d4/trl/runs/ox49wvqk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_config,\n",
    "    model=model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=dataset,\n",
    "    data_collator=dataset_collator,\n",
    "    # TODO: use learning rate scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in ppo_trainer.dataloader:\n",
    "#     print(batch[\"input_ids\"][0])\n",
    "#     print(ppo_trainer.generate(batch[\"input_ids\"][0]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BERT-based classification model to use as Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb8d2684c08472596a1ab09f294f639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "device = ppo_trainer.accelerator.device\n",
    "if ppo_trainer.accelerator.num_processes == 1:\n",
    "    if torch.cuda.is_available():\n",
    "        device = 0\n",
    "    else:\n",
    "        device = \"cpu\"  # although this is not feasible\n",
    "formality_pipe = pipeline(\n",
    "    task=\"text-classification\",\n",
    "    model=REWARD_MODEL_ID,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output directory setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dirpath = os.path.join(\"checkpoints\", checkpoint_dir_name)\n",
    "os.makedirs(checkpoint_dirpath, exist_ok=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define generation settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 50,  # NOTE: Here's room for optimization\n",
    "    \"top_p\": .95,  # NOTE: Here's room for optimization\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_min_length = 4\n",
    "output_max_length = 16\n",
    "output_length_sampler = LengthSampler(\n",
    "    min_value=output_min_length,\n",
    "    max_value=output_max_length,\n",
    ")\n",
    "\n",
    "EPOCH = 2\n",
    "\n",
    "batch_count = len(ppo_trainer.dataloader)\n",
    "save_interval = batch_count // 5\n",
    "for epoch in range(EPOCH):\n",
    "    for batch_pos, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "        queries_tensor = batch[\"input_ids\"]\n",
    "        \n",
    "        # retrieve response from text generator (faster implementation than tutorial)\n",
    "        # generation_kwargs[\"max_new_tokens\"] = output_max_length\n",
    "        # completions = ppo_trainer.generate(queries_tensor, **generation_kwargs)\n",
    "        # responses_tensor = []\n",
    "        # for i in range(completions.shape[0]):\n",
    "        #     generation_len = output_length_sampler()\n",
    "        #     responses_tensor.append(completions[i, :generation_len])\n",
    "        \n",
    "        completions_tensor = []\n",
    "        for query in queries_tensor:\n",
    "            generation_len = output_length_sampler()\n",
    "            generation_kwargs[\"max_new_tokens\"] = generation_len\n",
    "            completion = ppo_trainer.generate(query, **generation_kwargs)\n",
    "            completions_tensor.append(completion.squeeze()[-generation_len:])\n",
    "        batch[\"response\"] = [tokenizer.decode(c.squeeze()) for c in completions_tensor]\n",
    "        \n",
    "        # compute informality score\n",
    "        completed_texts = [q + c for q, c in zip(batch[\"query\"], batch[\"response\"])]\n",
    "        pipe_outputs = formality_pipe(completed_texts, **bert_kwargs)\n",
    "        # output[0] is a score for \"informal\"\n",
    "        rewards = [torch.tensor(output[0][\"score\"]) for output in pipe_outputs]\n",
    "\n",
    "        # execute PPO to tune parameter\n",
    "        stats = ppo_trainer.step(\n",
    "            queries=queries_tensor,\n",
    "            responses=completions_tensor,\n",
    "            scores=rewards,\n",
    "        )\n",
    "        ppo_trainer.log_stats(\n",
    "            stats=stats,\n",
    "            batch=batch,\n",
    "            rewards=rewards,\n",
    "        )\n",
    "\n",
    "        if batch_pos % save_interval == 0:\n",
    "            ppo_trainer.accelerator.save_model(\n",
    "                model=model,\n",
    "                save_directory=os.path.join(\n",
    "                    checkpoint_dirpath, f\"checkpoint-at-epoch-{epoch}-batch-{batch_pos}\"\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the model at the end of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_trainer.accelerator.save_model(\n",
    "    model=model,\n",
    "    save_directory=os.path.join(\n",
    "        checkpoint_dirpath, f\"checkpoint-at-epoch-{epoch}-batch-{batch_pos}\"\n",
    "    )\n",
    ")\n",
    "tokenizer.save_pretrained(\n",
    "    save_directory=os.path.join(checkpoint_dirpath, \"checkpoint-last\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"checkpoints/gpt2-medium-and-bert-based-formality-ranker-2023-07-20_01-53-49.109389/checkpoint-last\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"checkpoints/gpt2-medium-and-bert-based-formality-ranker-2023-07-20_01-53-49.109389/checkpoint-last\"\n",
    ")\n",
    "\n",
    "gpt_generation_pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    max_length=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_generation_pipe(\"He is \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
