{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental implementation of Informal-GPT\n",
    "\n",
    "This notebook will finetune GPT-2 medium model to talk in informal way.\n",
    "\n",
    "Will use\n",
    "\n",
    "- [s-nlp/roberta-base-formality-ranker](https://huggingface.co/s-nlp/roberta-base-formality-ranker) as a reward model\n",
    "- [robowaifudev/megatron-gpt2-345m](https://huggingface.co/robowaifudev/megatron-gpt2-345m) as a autoregressive text generator\n",
    "- [wikitext](https://huggingface.co/datasets/wikitext) as a query source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# Hugging Face transformers\n",
    "from transformers import pipeline, AutoTokenizer, PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# TRL library\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "# IDs of models to use\n",
    "REWARD_MODEL_ID = \"s-nlp/roberta-base-formality-ranker\"\n",
    "TEXT_GENERATION_MODEL_ID = \"robowaifudev/megatron-gpt2-345m\"\n",
    "# IDs of dataset to use\n",
    "DATASET_ID = \"wikitext\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: No names found, cannot describe anything.\n"
     ]
    }
   ],
   "source": [
    "GPT_BATCH_SIZE = 300\n",
    "BERT_BATCH_SIZE = 16\n",
    "\n",
    "# os.environ[\"WANDB_PROJECT\"] = \"informal-gpt\"\n",
    "ppo_config = PPOConfig(\n",
    "    batch_size=GPT_BATCH_SIZE,\n",
    "    model_name=TEXT_GENERATION_MODEL_ID,\n",
    "    learning_rate=1.41e-5,  # NOTE: This parameter is taken from OpenAI's paper: \"Fine-Tuning Language Models from Human Preferences\"\n",
    "    log_with=\"wandb\",\n",
    ")\n",
    "datetime_str = str(datetime.datetime.now()).replace(\" \", \"_\").replace(\":\", \"-\")\n",
    "checkpoint_dir_name = f\"megatron-gpt2-medium-and-bert-based-formality-ranker-{datetime_str}\"\n",
    "\n",
    "bert_kwargs = {\n",
    "    \"return_all_scores\": True,  # make BERT return scores for all classes, not only the most likely one\n",
    "    \"function_to_apply\": \"none\",\n",
    "    \"batch_size\": BERT_BATCH_SIZE,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "connect to wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mt0d4\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/toda/informal-gpt/wandb/run-20230719_020146-gsgvp7ni</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/t0d4/informal-gpt/runs/gsgvp7ni' target=\"_blank\">megatron-gpt2-medium-and-bert-based-formality-ranker</a></strong> to <a href='https://wandb.ai/t0d4/informal-gpt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/t0d4/informal-gpt' target=\"_blank\">https://wandb.ai/t0d4/informal-gpt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/t0d4/informal-gpt/runs/gsgvp7ni' target=\"_blank\">https://wandb.ai/t0d4/informal-gpt/runs/gsgvp7ni</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/t0d4/informal-gpt/runs/gsgvp7ni?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f8a26081460>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(\n",
    "    project=\"informal-gpt\",\n",
    "    name=f\"megatron-gpt2-medium-and-bert-based-formality-ranker-{datetime_str}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare **wikitext** dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define function for dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(\n",
    "    dataset_id: str,\n",
    "    dataset_size: int,\n",
    "    tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],\n",
    "    config_name: Optional[str] = None,\n",
    "    input_min_text_length: int = 4,\n",
    "    input_max_text_length: int = 8,\n",
    ") -> DataLoader:\n",
    "    \n",
    "    if config_name:\n",
    "        ds = load_dataset(dataset_id, config_name, split=\"train\")\n",
    "    else:\n",
    "        ds = load_dataset(\n",
    "            path=dataset_id,\n",
    "            split=\"train\",  # retrieve only train split\n",
    "        )\n",
    "    ds = ds.filter(\n",
    "        function=lambda item: len(item[\"text\"]) > 150,  # only use sentences whose length is more than 150 characters\n",
    "        batched=False,\n",
    "    )\n",
    "    \n",
    "    ds = ds.shuffle()\n",
    "    ds = ds[:dataset_size]\n",
    "    ds = Dataset.from_dict(ds)  # ds becomes `dict` when sliced, so change it back to Dataset\n",
    "    print(ds)\n",
    "    \n",
    "    def tokenize(item):\n",
    "        \"\"\"\n",
    "            item[\"input_ids\"] will be a sequence of first `get_input_size()` tokens converted from the input text.\n",
    "            item[\"query\"] will be a human-readable raw text decoded from item[\"input_ids\"]\n",
    "        \"\"\"\n",
    "        get_input_size = LengthSampler(\n",
    "            min_value=input_min_text_length,\n",
    "            max_value=input_max_text_length,\n",
    "        )\n",
    "        item[\"input_ids\"] = tokenizer.encode(text=item[\"text\"])[:get_input_size()]\n",
    "        item[\"query\"] = tokenizer.decode(token_ids=item[\"input_ids\"])\n",
    "        return item\n",
    "    \n",
    "    ds = ds.map(\n",
    "        function=tokenize,\n",
    "        batched=False,\n",
    "        num_proc=8,\n",
    "    )\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instantiate tokenizer and do preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.load:Using the latest cached version of the module from /home/toda/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126 (last modified on Tue Jul 18 23:01:28 2023) since it couldn't be found locally at wikitext., or remotely on the Hugging Face Hub.\n",
      "WARNING:datasets.builder:Found cached dataset wikitext (/home/toda/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/toda/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-aef69b5b372d6f2d.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 40000\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "294068a44bfe433cab3f83b04125e589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reference: imdb dataset's train split contains 24895 rows\n",
    "DATASET_SIZE = 24895\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=ppo_config.model_name,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # use <EOS> token for padding\n",
    "\n",
    "dataset = build_dataset(\n",
    "    dataset_id=DATASET_ID,\n",
    "    dataset_size = DATASET_SIZE,\n",
    "    config_name=\"wikitext-103-raw-v1\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "def dataset_collator(data: List[Dict[str, Any]]) -> Dict[str, List[Any]]:\n",
    "    # Data is like list of dictionaries.\n",
    "    # We format them as a batch\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained text generation models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    pretrained_model_name_or_path=ppo_config.model_name,\n",
    ")\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    pretrained_model_name_or_path=ppo_config.model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:gsgvp7ni) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">megatron-gpt2-medium-and-bert-based-formality-ranker</strong> at: <a href='https://wandb.ai/t0d4/informal-gpt/runs/gsgvp7ni' target=\"_blank\">https://wandb.ai/t0d4/informal-gpt/runs/gsgvp7ni</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230719_020146-gsgvp7ni/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:gsgvp7ni). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb65d0dafe954fed87dfa112869bb2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01667026193366231, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/toda/informal-gpt/wandb/run-20230719_020231-ox49wvqk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/t0d4/trl/runs/ox49wvqk' target=\"_blank\">proud-planet-3</a></strong> to <a href='https://wandb.ai/t0d4/trl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/t0d4/trl' target=\"_blank\">https://wandb.ai/t0d4/trl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/t0d4/trl/runs/ox49wvqk' target=\"_blank\">https://wandb.ai/t0d4/trl/runs/ox49wvqk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_config,\n",
    "    model=model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=dataset,\n",
    "    data_collator=dataset_collator,\n",
    "    # TODO: use learning rate scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in ppo_trainer.dataloader:\n",
    "#     print(batch[\"input_ids\"][0])\n",
    "#     print(ppo_trainer.generate(batch[\"input_ids\"][0]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BERT-based classification model to use as Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb8d2684c08472596a1ab09f294f639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "device = ppo_trainer.accelerator.device\n",
    "if ppo_trainer.accelerator.num_processes == 1:\n",
    "    if torch.cuda.is_available():\n",
    "        device = 0\n",
    "    else:\n",
    "        device = \"cpu\"  # although this is not feasible\n",
    "formality_pipe = pipeline(\n",
    "    task=\"text-classification\",\n",
    "    model=REWARD_MODEL_ID,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output directory setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dirpath = os.path.join(\"checkpoints\", checkpoint_dir_name)\n",
    "os.makedirs(checkpoint_dirpath, exist_ok=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define generation settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 50,  # NOTE: Here's room for optimization\n",
    "    \"top_p\": .95,  # NOTE: Here's room for optimization\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_min_length = 4\n",
    "output_max_length = 16\n",
    "output_length_sampler = LengthSampler(\n",
    "    min_value=output_min_length,\n",
    "    max_value=output_max_length,\n",
    ")\n",
    "\n",
    "batch_count = len(ppo_trainer.dataloader)\n",
    "save_interval = batch_count // 5\n",
    "for batch_pos, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    queries_tensor = batch[\"input_ids\"]\n",
    "    \n",
    "    # retrieve response from text generator (faster implementation than tutorial)\n",
    "    # generation_kwargs[\"max_new_tokens\"] = output_max_length\n",
    "    # completions = ppo_trainer.generate(queries_tensor, **generation_kwargs)\n",
    "    # responses_tensor = []\n",
    "    # for i in range(completions.shape[0]):\n",
    "    #     generation_len = output_length_sampler()\n",
    "    #     responses_tensor.append(completions[i, :generation_len])\n",
    "    \n",
    "    completions_tensor = []\n",
    "    for query in queries_tensor:\n",
    "        generation_len = output_length_sampler()\n",
    "        generation_kwargs[\"max_new_tokens\"] = generation_len\n",
    "        completion = ppo_trainer.generate(query, **generation_kwargs)\n",
    "        completions_tensor.append(completion.squeeze()[-generation_len:])\n",
    "    batch[\"response\"] = [tokenizer.decode(c.squeeze()) for c in completions_tensor]\n",
    "    \n",
    "    # compute informality score\n",
    "    completed_texts = [q + c for q, c in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    pipe_outputs = formality_pipe(completed_texts, **bert_kwargs)\n",
    "    # output[0] is a score for \"informal\"experiments/checkpoints/megatron-gpt2-medium-and-bert-based-formality-ranker-2023-07-19_10-20-47.357590/checkpoint-last\n",
    "    rewards = [torch.tensor(output[0][\"score\"]) for output in pipe_outputs]\n",
    "\n",
    "    # execute PPO to tune parameter\n",
    "    stats = ppo_trainer.step(\n",
    "        queries=queries_tensor,\n",
    "        responses=completions_tensor,\n",
    "        scores=rewards,\n",
    "    )\n",
    "    ppo_trainer.log_stats(\n",
    "        stats=stats,\n",
    "        batch=batch,\n",
    "        rewards=rewards,\n",
    "    )\n",
    "\n",
    "    if batch_pos % save_interval == 0:\n",
    "        model.save_pretrained(\n",
    "            save_directory=os.path.join(\n",
    "                checkpoint_dirpath, f\"checkpoint-at-step-{batch_pos}\"\n",
    "            )\n",
    "        )\n",
    "        tokenizer.save_pretrained(\n",
    "            save_directory=os.path.join(\n",
    "                checkpoint_dirpath, f\"checkpoint-at-step-{batch_pos}\"\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the model at the end of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\n",
    "    save_directory=os.path.join(checkpoint_dirpath, \"checkpoint-last\")\n",
    ")\n",
    "tokenizer.save_pretrained(\n",
    "    save_directory=os.path.join(checkpoint_dirpath, \"checkpoint-last\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at checkpoints/megatron-gpt2-medium-and-bert-based-formality-ranker-2023-07-19_10-20-47.380135/checkpoint-last were not used when initializing GPT2LMHeadModel: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'AutoModelForCausalLMWithValueHead' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"checkpoints/megatron-gpt2-medium-and-bert-based-formality-ranker-2023-07-19_10-20-47.380135/checkpoint-last\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"checkpoints/megatron-gpt2-medium-and-bert-based-formality-ranker-2023-07-19_10-20-47.380135/checkpoint-last\"\n",
    ")\n",
    "\n",
    "gpt_generation_pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    max_length=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'My laptop is just a little bit bit bit bit bit bit bit of a big-by-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_generation_pipe(\"My laptop is\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
